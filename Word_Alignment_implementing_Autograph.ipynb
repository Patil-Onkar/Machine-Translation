{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Alignment implementing Autograph.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfZ14huOrBNv"
      },
      "source": [
        "# Machine Translation\n",
        "  1. Using word alignment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2zRQogUQ3Xb"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwdlLZF9qyJK"
      },
      "source": [
        "# Loading Dataset\n",
        "ftr=open('/content/drive/My Drive/Projects/Machine Translation/small_vocab_fr','r')\n",
        "ftrain=ftr.read()\n",
        "ftrain=ftrain.split('\\n')\n",
        "ftr.close()\n",
        "etr=open('/content/drive/My Drive/Projects/Machine Translation/small_vocab_en','r')\n",
        "etrain=etr.read()\n",
        "etrain=etrain.split('\\n')\n",
        "etr.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Po5D-viPSwI"
      },
      "source": [
        "# Text preprocessing\r\n",
        "# Checking the sequence of wordsfor padding.\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "tokenizer_french=tf.keras.preprocessing.text.Tokenizer(num_words=10000 , oov_token='<oov>')\r\n",
        "tokenizer_french.fit_on_texts(ftrain)\r\n",
        "fseq=tokenizer_french.texts_to_sequences(ftrain)\r\n",
        "\r\n",
        "tokenizer_eng=tf.keras.preprocessing.text.Tokenizer(num_words=10000,oov_token='<oov>')\r\n",
        "tokenizer_eng.fit_on_texts(etrain)\r\n",
        "eseq=tokenizer_eng.texts_to_sequences(etrain)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "LC1fAELRPskW",
        "outputId": "663f08d0-282f-4467-e7ec-71dffa869d5d"
      },
      "source": [
        "# Checking the length of sentences\r\n",
        "fr=[len(i) for i in fseq]\r\n",
        "en=[len(i) for i in eseq]\r\n",
        "print('mean length for french sentence ={}, for english={}'.format(np.mean(fr),np.mean(en)))\r\n",
        "print('STD for french sentence length ={}, for english={}'.format(np.std(fr),np.std(en)))\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.hist(fr,bins=10)\r\n",
        "plt.show()\r\n",
        "plt.hist(en,bins=10)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# max len \\\r\n",
        "print('\\n\\nMax len for french ={} , for english {}'.format(max(fr),max(en)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean length for french sentence =12.423528046365542, for english=11.261364707930452\n",
            "STD for french sentence length =2.867754490213468, for english=3.172094299930908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Max len for french =21 , for english 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iosXAG8GPsak",
        "outputId": "b89aca9a-f9e6-4194-f65b-136f12dd1eea"
      },
      "source": [
        "# We can pad the sequence upto its max length\r\n",
        "\r\n",
        "fpad=tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    fseq, maxlen=None, dtype='int32', padding='post', truncating='post',\r\n",
        "    value=0.0)\r\n",
        "\r\n",
        "epad=tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    eseq, maxlen=None, dtype='int32', padding='post', truncating='post',\r\n",
        "    value=0.0)\r\n",
        "\r\n",
        "\r\n",
        "# Lets check the vocab length\r\n",
        "\r\n",
        "print('Length of french vocab {} and english vocab {}'.format(len(tokenizer_french.word_index),len(tokenizer_eng.word_index)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of french vocab 345 and english vocab 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JECtamAdcAQb",
        "outputId": "b4a1df1f-5923-4282-bd66-02da68155af5"
      },
      "source": [
        "# Add pad and start word.\r\n",
        "\r\n",
        "# In english vocab add <pad>  token \r\n",
        "tokenizer_eng.word_index['<pad>'] = 0\r\n",
        "\r\n",
        "\r\n",
        "# In french vocab add <pad> and <start> token \r\n",
        "tokenizer_french.word_index['<pad>']=0\r\n",
        "tokenizer_french.word_index['<start>'] = 346\r\n",
        "\r\n",
        "print('Now the size of the vocab is now, for french {} and eng {}'.format(len(tokenizer_french.word_index),len(tokenizer_eng.word_index)))\r\n",
        "\r\n",
        "# Create reverse Dictionary \r\n",
        "\r\n",
        "fr_itow={i:j for j,i in tokenizer_french.word_index.items()}\r\n",
        "en_itow={i:j for j,i in tokenizer_eng.word_index.items()}\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now the size of the vocab is now, for french 347 and eng 201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKrqWIVnsk_Z"
      },
      "source": [
        "# Creating train and validation set \n",
        "\n",
        "# Train\n",
        "train_input=epad[:130000].reshape((130000,15))\n",
        "train_output=fpad[:130000].reshape((130000,21))\n",
        "# val\n",
        "val_input=epad[130000:].reshape((7861,15))\n",
        "val_output=fpad[130000:].reshape((7861,21))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMaGwS1chVJE"
      },
      "source": [
        "# Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuqYnOJvtLVX"
      },
      "source": [
        "Implementing the paper's archetacture without the alignment layer that is without attention. \n",
        "\n",
        "Paper- NEURAL MACHINE TRANSLATION\n",
        "BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\n",
        "by - Dzmitry Bahdanau\n",
        "\n",
        "\n",
        "  1. The Encoder -Decoder architecture is as follows-\n",
        "\n",
        "  Encoder = Input --> Embedding with 16 vector size ---> Bi-Directional lstm with 15 units.\n",
        "    \n",
        "  Decoder = Output --> Bidirectional lstm 21 units--> softmax Dense\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFzU4xWOhxUF"
      },
      "source": [
        "# Encoder\r\n",
        "\r\n",
        "# Building encoder with bidirectional lstm\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "class Encoder(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(Encoder,self).__init__()\r\n",
        "    self.em=tf.keras.layers.Embedding(201,16)\r\n",
        "    self.drop=tf.keras.layers.Dropout(0.2)\r\n",
        "    self.drop1=tf.keras.layers.Dropout(0.2)\r\n",
        "    self.enc=tf.keras.layers.Bidirectional(tf.keras.layers.GRU(15,activation='relu',return_state=True,return_sequences=True))\r\n",
        "    #self.enc1=tf.keras.layers.Bidirectional(tf.keras.layers.GRU(15,activation='relu',return_state=True))\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def call(self,inputs):\r\n",
        "    emb=self.em(inputs)\r\n",
        "    d1=self.drop(emb)\r\n",
        "    en1=self.enc(d1)\r\n",
        "    #d2=self.drop1(en1[0])\r\n",
        "    #en2=self.enc1(d2)\r\n",
        "    #out=tf.reshape(en1[0],(en1[0].shape[0],1,30))\r\n",
        "    hid=tf.concat([en1[1],en1[2]],axis=1)\r\n",
        "    return en1[0],hid\r\n",
        "\r\n",
        "enc=Encoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoiV9BkHjN9z",
        "outputId": "f4193194-8df8-4699-e877-6a73025c0144"
      },
      "source": [
        "# Testing the input\r\n",
        "# Required output shape==> (batch,maxlen,hidden_state)  ==(2,15,30)\r\n",
        "inp=train_input[:2]\r\n",
        "#o=enc(inp)\r\n",
        "out,hid=enc(inp)\r\n",
        "assert out.shape==(2,15,30), 'Somthing went wrong with out'\r\n",
        "assert hid.shape==(2,30), 'Somthing went wrong with hidden output'\r\n",
        "print('looks fine')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks fine\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XabxvzFknlpp"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\r\n",
        "  def __init__(self,units):\r\n",
        "    super(BahdanauAttention,self).__init__()\r\n",
        "    self.w1=tf.keras.layers.Dense(units)\r\n",
        "    self.w2=tf.keras.layers.Dense(units)\r\n",
        "    self.v=tf.keras.layers.Dense(1)\r\n",
        "    self.activation = tf.keras.layers.Activation('tanh')\r\n",
        "    #self.activation1 = tf.keras.layers.Activation('softmax')\r\n",
        "    #self.soft=tf.keras.activations.softmax(axis=1)\r\n",
        "\r\n",
        "  #@tf.function\r\n",
        "  def call(self,dec_hid,enc_out):\r\n",
        "    x=self.w1(dec_hid)+self.w2(enc_out)\r\n",
        "    x=self.activation(x)\r\n",
        "    x=self.v(x)\r\n",
        "    wghts=tf.keras.activations.softmax(x,axis=1)\r\n",
        "    context_vector=wghts*enc_out\r\n",
        "    context_vector=tf.reduce_sum(context_vector,axis=1)\r\n",
        "    return wghts,context_vector\r\n",
        "\r\n",
        "\r\n",
        "#units=30\r\n",
        "at=BahdanauAttention(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-64yrgjClkcN",
        "outputId": "b8129b84-f459-4b99-ed18-7a7a85925c5f"
      },
      "source": [
        "#Testing bahdanau's attention word alignment\r\n",
        "\r\n",
        "init=tf.random_normal_initializer()\r\n",
        "dec_hid=tf.Variable(initial_value=init(shape=(2,1,30),dtype=tf.float32))\r\n",
        "enc_out=out\r\n",
        "\r\n",
        "# Expected weights output shape ==> (batch,maxlen,1) ==> (2,15,1)\r\n",
        "# Expected context vector output shape ==> (batch,hidden) ==> (2,15)\r\n",
        "weights,context_vector=at(dec_hid,enc_out)\r\n",
        "assert weights.shape==(2,15,1), 'Somthing went wrong with weights'\r\n",
        "assert context_vector.shape==(2,30), 'Somthing went wrong with context_vector'\r\n",
        "print('looks fine')\r\n",
        "\r\n",
        "# Printing the weights\r\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks fine\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 15, 1), dtype=float32, numpy=\n",
              "array([[[0.06712859],\n",
              "        [0.06688886],\n",
              "        [0.06586035],\n",
              "        [0.06565526],\n",
              "        [0.06592856],\n",
              "        [0.06552867],\n",
              "        [0.06654438],\n",
              "        [0.06677392],\n",
              "        [0.06703028],\n",
              "        [0.06649968],\n",
              "        [0.06706674],\n",
              "        [0.06823452],\n",
              "        [0.06702882],\n",
              "        [0.06674438],\n",
              "        [0.06708697]],\n",
              "\n",
              "       [[0.06793259],\n",
              "        [0.06654064],\n",
              "        [0.0663745 ],\n",
              "        [0.06598224],\n",
              "        [0.06619244],\n",
              "        [0.06525595],\n",
              "        [0.06517455],\n",
              "        [0.06740995],\n",
              "        [0.06716557],\n",
              "        [0.06736331],\n",
              "        [0.06683553],\n",
              "        [0.0668715 ],\n",
              "        [0.06692804],\n",
              "        [0.06736183],\n",
              "        [0.06661135]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUFJpJiP1uTq"
      },
      "source": [
        "# Decoder\r\n",
        "# As we are applying teacher forcing, we can't use Bidirectional layer\r\n",
        "\r\n",
        "class Decoder(tf.keras.Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(Decoder,self).__init__()\r\n",
        "    self.em=tf.keras.layers.Embedding(347,16)\r\n",
        "    self.drop=tf.keras.layers.Dropout(0.2)\r\n",
        "    #self.drop1=tf.keras.layers.Dropout(0.2)\r\n",
        "    self.dec=tf.keras.layers.GRU(30,activation='relu',return_state=True,return_sequences=True)\r\n",
        "    #self.dec1=tf.keras.layers.GRU(21,activation='relu',return_state=True,return_sequences=True)\r\n",
        "    self.dense=tf.keras.layers.Dense(347,activation='sigmoid')\r\n",
        "    self.attention=BahdanauAttention(30)\r\n",
        "  @tf.function\r\n",
        "  def call(self,enc_out,dec_hidden,inp):\r\n",
        "    x=self.em(inp)\r\n",
        "    weights,context_vector=self.attention(dec_hidden,enc_out)\r\n",
        "    context_vector=tf.reshape(context_vector,shape=(context_vector.shape[0],1,context_vector.shape[-1]))\r\n",
        "    concat=tf.keras.layers.concatenate([x,context_vector,dec_hidden],axis=2)\r\n",
        "    #drop=self.drop(concat)\r\n",
        "    dec_out,dec_hidden=self.dec(concat)\r\n",
        "    #drop1=self.drop1(dec_out)\r\n",
        "    #dec_hidden2=tf.reshape(dec_hidden2,shape=(dec_hidden2.shape[0],1,dec_hidden2.shape[-1]))\r\n",
        "    #concat1=tf.keras.layers.concatenate([dec_out,dec_hidden2],axis=2)\r\n",
        "    #drop1=self.drop1(concat1)\r\n",
        "    #dec_out2,dec_hidden2=self.dec1(drop1)\r\n",
        "    pred=self.dense(dec_out)\r\n",
        "    return dec_hidden,pred,weights\r\n",
        "\r\n",
        "dec=Decoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAuqGnlu_5_2",
        "outputId": "7e0780c7-14a2-4994-9aca-10e28bdc8f18"
      },
      "source": [
        "#Testing decoder\r\n",
        "\r\n",
        "# Output shapes should be : dec_hidden=(batch,1,hidden)==>(2,1,21),weights=(batch,maxlen,1)==>(2,15,1)\r\n",
        "#                           pred=(batch,1,vocab)==>(2,1,347) \r\n",
        "\r\n",
        "inp=np.array([346,346]).reshape((2,1))\r\n",
        "dec_hidden,pred,weights=dec(enc_out,dec_hid,inp)\r\n",
        "\r\n",
        "assert weights.shape==(2,15,1), 'Somthing went wrong with weights'\r\n",
        "assert dec_hidden.shape==(2,30), 'Somthing went wrong with decoder hidden'\r\n",
        "assert pred.shape==(2,1,347), 'Somthing went wrong with prediction vector'\r\n",
        "\r\n",
        "print('looks good')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Ia5xV1G2lr"
      },
      "source": [
        "# Creating required APIs\r\n",
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "loss_ = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "acc=tf.keras.metrics.Accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIqe5tF6bjIB"
      },
      "source": [
        "# with autograph\r\n",
        "\r\n",
        "class Test_step():\r\n",
        "  def __init__(self,batch):\r\n",
        "    self.batch=batch\r\n",
        "    #initializer=tf.random_normal_initializer(seed=33)\r\n",
        "    #self.dec_hidden=tf.Variable(initial_value=initializer(shape=(batch,1,21),dtype=tf.float32))\r\n",
        "    #self.dec_hidden=tf.ones(shape=(batch,1,21))\r\n",
        "    self.dec_input=np.array([346]*batch).reshape(batch,1)\r\n",
        "    self.pred_out=[]\r\n",
        "    self.wghts=[]\r\n",
        "    self.loss=0\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def  __call__(self,inp,out):\r\n",
        "    dec_inp=self.dec_input\r\n",
        "    #dec_hid=self.dec_hidden\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "      enc_out,dec_hid=enc(inp)\r\n",
        "      dec_hid=tf.reshape(dec_hid,shape=(self.batch,1,dec_hid.shape[-1]))\r\n",
        "      for i in range(out.shape[1]):\r\n",
        "        dec_hid,pred,weights=dec(enc_out,dec_hid,dec_inp)\r\n",
        "        dec_inp=tf.reshape(out[:,i],shape=(self.batch,1))\r\n",
        "        dec_hid=tf.reshape(dec_hid,shape=(self.batch,1,dec_hid.shape[-1]))\r\n",
        "        self.pred_out.append(pred)\r\n",
        "        self.wghts.append(weights)\r\n",
        "\r\n",
        "      ypred=tf.concat(self.pred_out,axis=1)\r\n",
        "      weight=tf.concat(self.wghts,axis=2)\r\n",
        "      ls=loss_(out,ypred)\r\n",
        "\r\n",
        "    variables = enc.trainable_variables + dec.trainable_variables\r\n",
        "    gradients = tape.gradient(ls, variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "    #self.dec_hidden=dec_hid\r\n",
        "    return ls,ypred,weight\r\n",
        "\r\n",
        "\r\n",
        "#step=Test_step(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg1NWYeKMkHO",
        "outputId": "d8132226-0051-4680-a5f6-1ae2fdcf71e3"
      },
      "source": [
        "# checking if training is actually happening or not\r\n",
        "'''\r\n",
        "batch=30\r\n",
        "initializer=tf.random_normal_initializer(seed=33)\r\n",
        "dec_hid=tf.Variable(initial_value=initializer(shape=(batch,1,21),dtype=tf.float32))\r\n",
        "'''\r\n",
        "\r\n",
        "# Test 1\r\n",
        "step=Test_step(30)\r\n",
        "inp=train_input[:30]\r\n",
        "out=train_output[:30]\r\n",
        "\r\n",
        "ls,ypred,weights=step(inp,out)\r\n",
        "print(ls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.849387, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VcaL2IVNtID",
        "outputId": "24e2f600-8632-43bf-a6b7-528142e79401"
      },
      "source": [
        "# Test 2\r\n",
        "inp=train_input[30:60]\r\n",
        "out=train_output[30:60]\r\n",
        "\r\n",
        "ls,ypred,weights=step(inp,out)\r\n",
        "print(ls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.8483667, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyLupSamOpoy",
        "outputId": "336b3bb7-a03b-47ea-8b00-c9cb066906ee"
      },
      "source": [
        "# Test 3\r\n",
        "inp=train_input[60:90]\r\n",
        "out=train_output[60:90]\r\n",
        "\r\n",
        "ls,ypred,weights=step(inp,out)\r\n",
        "print(ls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.8472323, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC-PMp0pk81r"
      },
      "source": [
        "class Prediction():\r\n",
        "  def __init__(self,batch):\r\n",
        "    self.batch=batch\r\n",
        "    #initializer=tf.random_normal_initializer(seed=33)\r\n",
        "    #self.dec_hidden=tf.Variable(initial_value=initializer(shape=(batch,1,21),dtype=tf.float32))\r\n",
        "    self.dec_input=np.array([345]*batch).reshape(batch,1)\r\n",
        "    self.pred_out=[]\r\n",
        "    self.wghts=[]\r\n",
        "\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def __call__(self,inp):\r\n",
        "    dec_inp=self.dec_input\r\n",
        "    #dec_hid=self.dec_hidden\r\n",
        "    enc_out,dec_hid=enc(inp)\r\n",
        "    dec_hid=tf.reshape(dec_hid,shape=(self.batch,1,dec_hid.shape[-1]))\r\n",
        "    for i in range(21):\r\n",
        "      dec_hid,pred,weights=dec(enc_out,dec_hid,dec_inp)\r\n",
        "      dec_inp=tf.reshape(tf.math.argmax(pred,axis=2),shape=(self.batch,1))\r\n",
        "      dec_hid=tf.reshape(dec_hid,shape=(self.batch,1,dec_hid.shape[-1]))\r\n",
        "      self.pred_out.append(pred)\r\n",
        "      self.wghts.append(weights)\r\n",
        "    weight=tf.concat(self.wghts,axis=2)\r\n",
        "    ypred=tf.concat(self.pred_out,axis=1)\r\n",
        "    return ypred,weight\r\n",
        "\r\n",
        "\r\n",
        "predict=Prediction(val_input.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QG2KQaRaBLb"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvd-zL0AXT7T",
        "outputId": "b3943db2-84c3-451b-e774-45061fc55f9a"
      },
      "source": [
        "# Testing the predict function\r\n",
        "\r\n",
        "# Output shapes should be : weights=(batch,maxlen_inp,maxlen_output)==>(5,15,21)\r\n",
        "#                           ypred=(batch,21,vocab)==>(5,21,347) \r\n",
        "\r\n",
        "inp=train_input[:5]\r\n",
        "ypred,weight=predict(val_input)\r\n",
        "assert weight.shape==(val_input.shape[0],15,21), 'Somthing went wrong with weights'\r\n",
        "assert ypred.shape==(val_input.shape[0],21,347) , 'Somthing went wrong with prediction vector'\r\n",
        "\r\n",
        "print('looks good')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5wFRWu_YZeS",
        "outputId": "a7ea3010-0237-453b-f7c6-fd21a1aa8eba"
      },
      "source": [
        "# Lets train the model for 20 epochs\r\n",
        "import time \r\n",
        "# Training...\r\n",
        "batch=64\r\n",
        "step=Test_step(batch)\r\n",
        "trn=tf.data.Dataset.from_tensor_slices((train_input,train_output)).batch(batch,drop_remainder=True)\r\n",
        "epochs=15\r\n",
        "print('Training starts ...')\r\n",
        "for j in range(epochs):\r\n",
        "  t=time.time()\r\n",
        "  print('\\n\\n---Epoch {}  :----'.format(j))\r\n",
        "  a=[]\r\n",
        "  l=[]\r\n",
        "  for i,(inp,out) in enumerate(trn):\r\n",
        "    ls,ypred,weight=step(inp,out)\r\n",
        "    ypred=tf.math.argmax(ypred,axis=2)\r\n",
        "    ypred=tf.reshape(ypred,shape=out.shape)\r\n",
        "    ac=acc(ypred,out)\r\n",
        "    l.append(ls)\r\n",
        "    a.append(ac)\r\n",
        "    if (i+1)%10==0:\r\n",
        "      #print('At {}th batch    mean_loss={} , mean_accuracy={}'.format(i,np.mean(l),np.mean(a)))\r\n",
        "      ma=np.mean(a)\r\n",
        "      ml=np.mean(l)\r\n",
        "      a=[]\r\n",
        "      l=[]\r\n",
        "    if (i+1)%100==0:\r\n",
        "      print('At {}th batch    mean_loss={} , mean_accuracy={}'.format(i+1,ml,ma))\r\n",
        "  ypred,weight=predict(val_input)\r\n",
        "  val_loss=loss_(val_output,ypred)*30/val_input.shape[0]\r\n",
        "  ypred=tf.math.argmax(ypred,axis=2)\r\n",
        "  ypred=tf.reshape(ypred,shape=val_output.shape)\r\n",
        "  val_ac=acc(ypred,val_output)\r\n",
        "  t=time.time()-t\r\n",
        "  print('\\nTime required for {} th epoch : {}\\nValidation loss :{} Validation Accuracy: {} '.format(j,t,val_loss,val_ac))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training starts ...\n",
            "\n",
            "\n",
            "---Epoch 0  :----\n",
            "At 100th batch    mean_loss=3.8584084510803223 , mean_accuracy=0.40468254685401917\n",
            "At 200th batch    mean_loss=3.5639801025390625 , mean_accuracy=0.3940236568450928\n",
            "At 300th batch    mean_loss=3.1806960105895996 , mean_accuracy=0.39153456687927246\n",
            "At 400th batch    mean_loss=2.3619706630706787 , mean_accuracy=0.3893580436706543\n",
            "At 500th batch    mean_loss=2.1648576259613037 , mean_accuracy=0.35948532819747925\n",
            "At 600th batch    mean_loss=2.086163282394409 , mean_accuracy=0.33200526237487793\n",
            "At 700th batch    mean_loss=1.9468361139297485 , mean_accuracy=0.3091089725494385\n",
            "At 800th batch    mean_loss=1.4508628845214844 , mean_accuracy=0.2817476689815521\n",
            "At 900th batch    mean_loss=0.9485357403755188 , mean_accuracy=0.32244259119033813\n",
            "At 1000th batch    mean_loss=0.8821623921394348 , mean_accuracy=0.3602973520755768\n",
            "At 1100th batch    mean_loss=0.8601306080818176 , mean_accuracy=0.39239391684532166\n",
            "At 1200th batch    mean_loss=0.8369793891906738 , mean_accuracy=0.41996270418167114\n",
            "At 1300th batch    mean_loss=0.781274676322937 , mean_accuracy=0.44402948021888733\n",
            "At 1400th batch    mean_loss=0.7505793571472168 , mean_accuracy=0.46506214141845703\n",
            "At 1500th batch    mean_loss=0.7058717608451843 , mean_accuracy=0.4837586283683777\n",
            "At 1600th batch    mean_loss=0.7071987390518188 , mean_accuracy=0.5014508962631226\n",
            "At 1700th batch    mean_loss=0.688807487487793 , mean_accuracy=0.5171929597854614\n",
            "At 1800th batch    mean_loss=0.657954216003418 , mean_accuracy=0.5315230488777161\n",
            "At 1900th batch    mean_loss=0.6533061265945435 , mean_accuracy=0.5445457100868225\n",
            "At 2000th batch    mean_loss=0.6253310441970825 , mean_accuracy=0.5565580725669861\n",
            "\n",
            "Time required for 0 th epoch : 101.15938234329224\n",
            "Validation loss :0.012622302398085594 Validation Accuracy: 0.5628665089607239 \n",
            "\n",
            "\n",
            "---Epoch 1  :----\n",
            "At 100th batch    mean_loss=0.6241039633750916 , mean_accuracy=0.5723456144332886\n",
            "At 200th batch    mean_loss=0.6132600903511047 , mean_accuracy=0.5814663767814636\n",
            "At 300th batch    mean_loss=0.6308042407035828 , mean_accuracy=0.59004807472229\n",
            "At 400th batch    mean_loss=0.5883849263191223 , mean_accuracy=0.5980516672134399\n",
            "At 500th batch    mean_loss=0.5997998118400574 , mean_accuracy=0.6055315732955933\n",
            "At 600th batch    mean_loss=0.6035500168800354 , mean_accuracy=0.6125367283821106\n",
            "At 700th batch    mean_loss=0.5771907567977905 , mean_accuracy=0.6191582679748535\n",
            "At 800th batch    mean_loss=0.583789050579071 , mean_accuracy=0.6253575086593628\n",
            "At 900th batch    mean_loss=0.5699678063392639 , mean_accuracy=0.6311483383178711\n",
            "At 1000th batch    mean_loss=0.5582440495491028 , mean_accuracy=0.6366382837295532\n",
            "At 1100th batch    mean_loss=0.5674756169319153 , mean_accuracy=0.641883134841919\n",
            "At 1200th batch    mean_loss=0.5631567239761353 , mean_accuracy=0.6468471884727478\n",
            "At 1300th batch    mean_loss=0.5620633363723755 , mean_accuracy=0.6515011191368103\n",
            "At 1400th batch    mean_loss=0.5485718250274658 , mean_accuracy=0.6559525728225708\n",
            "At 1500th batch    mean_loss=0.5263713598251343 , mean_accuracy=0.6602026224136353\n",
            "At 1600th batch    mean_loss=0.5454082489013672 , mean_accuracy=0.6642928123474121\n",
            "At 1700th batch    mean_loss=0.5392580032348633 , mean_accuracy=0.6682135462760925\n",
            "At 1800th batch    mean_loss=0.5294901728630066 , mean_accuracy=0.6718990802764893\n",
            "At 1900th batch    mean_loss=0.5250248908996582 , mean_accuracy=0.675484299659729\n",
            "At 2000th batch    mean_loss=0.5155191421508789 , mean_accuracy=0.6789062023162842\n",
            "\n",
            "Time required for 1 th epoch : 92.9081974029541\n",
            "Validation loss :0.011429990641772747 Validation Accuracy: 0.6794743537902832 \n",
            "\n",
            "\n",
            "---Epoch 2  :----\n",
            "At 100th batch    mean_loss=0.5183296203613281 , mean_accuracy=0.6824879050254822\n",
            "At 200th batch    mean_loss=0.5059226751327515 , mean_accuracy=0.6855741739273071\n",
            "At 300th batch    mean_loss=0.5331833958625793 , mean_accuracy=0.6884955763816833\n",
            "At 400th batch    mean_loss=0.5016816258430481 , mean_accuracy=0.6913463473320007\n",
            "At 500th batch    mean_loss=0.5043703317642212 , mean_accuracy=0.6941331624984741\n",
            "At 600th batch    mean_loss=0.5167826414108276 , mean_accuracy=0.6967672109603882\n",
            "At 700th batch    mean_loss=0.49688392877578735 , mean_accuracy=0.6993353962898254\n",
            "At 800th batch    mean_loss=0.5021156668663025 , mean_accuracy=0.7018280625343323\n",
            "At 900th batch    mean_loss=0.4864899516105652 , mean_accuracy=0.7042239308357239\n",
            "At 1000th batch    mean_loss=0.4895806312561035 , mean_accuracy=0.7065335512161255\n",
            "At 1100th batch    mean_loss=0.4957418441772461 , mean_accuracy=0.7087868452072144\n",
            "At 1200th batch    mean_loss=0.4950987696647644 , mean_accuracy=0.7109802961349487\n",
            "At 1300th batch    mean_loss=0.48395830392837524 , mean_accuracy=0.7131179571151733\n",
            "At 1400th batch    mean_loss=0.48017391562461853 , mean_accuracy=0.7151801586151123\n",
            "At 1500th batch    mean_loss=0.4633646011352539 , mean_accuracy=0.7172207832336426\n",
            "At 1600th batch    mean_loss=0.47912663221359253 , mean_accuracy=0.7192243933677673\n",
            "At 1700th batch    mean_loss=0.47591090202331543 , mean_accuracy=0.721164882183075\n",
            "At 1800th batch    mean_loss=0.4662324786186218 , mean_accuracy=0.7230123281478882\n",
            "At 1900th batch    mean_loss=0.4597470760345459 , mean_accuracy=0.7248358726501465\n",
            "At 2000th batch    mean_loss=0.460289865732193 , mean_accuracy=0.7266320586204529\n",
            "\n",
            "Time required for 2 th epoch : 95.52859115600586\n",
            "Validation loss :0.0110716437920928 Validation Accuracy: 0.7263053059577942 \n",
            "\n",
            "\n",
            "---Epoch 3  :----\n",
            "At 100th batch    mean_loss=0.46463480591773987 , mean_accuracy=0.7279173135757446\n",
            "At 200th batch    mean_loss=0.45411181449890137 , mean_accuracy=0.7295923233032227\n",
            "At 300th batch    mean_loss=0.4741215705871582 , mean_accuracy=0.7312105894088745\n",
            "At 400th batch    mean_loss=0.45018354058265686 , mean_accuracy=0.7328108549118042\n",
            "At 500th batch    mean_loss=0.4461655616760254 , mean_accuracy=0.7343848347663879\n",
            "At 600th batch    mean_loss=0.4625317454338074 , mean_accuracy=0.7359050512313843\n",
            "At 700th batch    mean_loss=0.43798279762268066 , mean_accuracy=0.7374139428138733\n",
            "At 800th batch    mean_loss=0.4441034197807312 , mean_accuracy=0.738883376121521\n",
            "At 900th batch    mean_loss=0.42791828513145447 , mean_accuracy=0.7403247952461243\n",
            "At 1000th batch    mean_loss=0.43632403016090393 , mean_accuracy=0.7417363524436951\n",
            "At 1100th batch    mean_loss=0.44623002409935 , mean_accuracy=0.7431102991104126\n",
            "At 1200th batch    mean_loss=0.4393865466117859 , mean_accuracy=0.7444542050361633\n",
            "At 1300th batch    mean_loss=0.4229256212711334 , mean_accuracy=0.7458003759384155\n",
            "At 1400th batch    mean_loss=0.420762836933136 , mean_accuracy=0.7471164464950562\n",
            "At 1500th batch    mean_loss=0.4138699471950531 , mean_accuracy=0.7484162449836731\n",
            "At 1600th batch    mean_loss=0.4269527494907379 , mean_accuracy=0.7497022747993469\n",
            "At 1700th batch    mean_loss=0.4203391969203949 , mean_accuracy=0.7509485483169556\n",
            "At 1800th batch    mean_loss=0.41488465666770935 , mean_accuracy=0.7521821856498718\n",
            "At 1900th batch    mean_loss=0.40385621786117554 , mean_accuracy=0.7533965706825256\n",
            "At 2000th batch    mean_loss=0.411810964345932 , mean_accuracy=0.754594624042511\n",
            "\n",
            "Time required for 3 th epoch : 95.29919576644897\n",
            "Validation loss :0.01083143800497055 Validation Accuracy: 0.7541019320487976 \n",
            "\n",
            "\n",
            "---Epoch 4  :----\n",
            "At 100th batch    mean_loss=0.4024219512939453 , mean_accuracy=0.7552127838134766\n",
            "At 200th batch    mean_loss=0.3981790542602539 , mean_accuracy=0.756369948387146\n",
            "At 300th batch    mean_loss=0.41504302620887756 , mean_accuracy=0.7574813365936279\n",
            "At 400th batch    mean_loss=0.39173412322998047 , mean_accuracy=0.7586056590080261\n",
            "At 500th batch    mean_loss=0.3878752887248993 , mean_accuracy=0.7597187161445618\n",
            "At 600th batch    mean_loss=0.40450963377952576 , mean_accuracy=0.7608058452606201\n",
            "At 700th batch    mean_loss=0.38217905163764954 , mean_accuracy=0.7619035243988037\n",
            "At 800th batch    mean_loss=0.38502365350723267 , mean_accuracy=0.7629749774932861\n",
            "At 900th batch    mean_loss=0.36982545256614685 , mean_accuracy=0.7640363574028015\n",
            "At 1000th batch    mean_loss=0.3772284984588623 , mean_accuracy=0.7650753259658813\n",
            "At 1100th batch    mean_loss=0.38427454233169556 , mean_accuracy=0.7661099433898926\n",
            "At 1200th batch    mean_loss=0.3764716684818268 , mean_accuracy=0.7671397924423218\n",
            "At 1300th batch    mean_loss=0.3613467812538147 , mean_accuracy=0.7681662440299988\n",
            "At 1400th batch    mean_loss=0.35853609442710876 , mean_accuracy=0.7691602110862732\n",
            "At 1500th batch    mean_loss=0.34786662459373474 , mean_accuracy=0.7701565623283386\n",
            "At 1600th batch    mean_loss=0.365145206451416 , mean_accuracy=0.7711704969406128\n",
            "At 1700th batch    mean_loss=0.35867229104042053 , mean_accuracy=0.7721644639968872\n",
            "At 1800th batch    mean_loss=0.3492512106895447 , mean_accuracy=0.77314293384552\n",
            "At 1900th batch    mean_loss=0.3434011936187744 , mean_accuracy=0.7741152048110962\n",
            "At 2000th batch    mean_loss=0.3427729904651642 , mean_accuracy=0.7750973701477051\n",
            "\n",
            "Time required for 4 th epoch : 95.29346704483032\n",
            "Validation loss :0.009679879993200302 Validation Accuracy: 0.7748473286628723 \n",
            "\n",
            "\n",
            "---Epoch 5  :----\n",
            "At 100th batch    mean_loss=0.340362548828125 , mean_accuracy=0.7757495045661926\n",
            "At 200th batch    mean_loss=0.32597267627716064 , mean_accuracy=0.7767085433006287\n",
            "At 300th batch    mean_loss=0.3423590064048767 , mean_accuracy=0.7776512503623962\n",
            "At 400th batch    mean_loss=0.31931453943252563 , mean_accuracy=0.7786017656326294\n",
            "At 500th batch    mean_loss=0.3255089223384857 , mean_accuracy=0.7795599102973938\n",
            "At 600th batch    mean_loss=0.33146122097969055 , mean_accuracy=0.780494213104248\n",
            "At 700th batch    mean_loss=0.31859904527664185 , mean_accuracy=0.7814487814903259\n",
            "At 800th batch    mean_loss=0.30870118737220764 , mean_accuracy=0.7823764681816101\n",
            "At 900th batch    mean_loss=0.29076579213142395 , mean_accuracy=0.7833267450332642\n",
            "At 1000th batch    mean_loss=0.2952374517917633 , mean_accuracy=0.7842724919319153\n",
            "At 1100th batch    mean_loss=0.29571664333343506 , mean_accuracy=0.785224974155426\n",
            "At 1200th batch    mean_loss=0.29620593786239624 , mean_accuracy=0.7861678004264832\n",
            "At 1300th batch    mean_loss=0.2775316834449768 , mean_accuracy=0.7871142625808716\n",
            "At 1400th batch    mean_loss=0.27703654766082764 , mean_accuracy=0.7880617380142212\n",
            "At 1500th batch    mean_loss=0.26691120862960815 , mean_accuracy=0.7889972925186157\n",
            "At 1600th batch    mean_loss=0.2805212140083313 , mean_accuracy=0.7899459600448608\n",
            "At 1700th batch    mean_loss=0.2682564854621887 , mean_accuracy=0.7909018397331238\n",
            "At 1800th batch    mean_loss=0.2626990079879761 , mean_accuracy=0.7918650507926941\n",
            "At 1900th batch    mean_loss=0.25565022230148315 , mean_accuracy=0.7928023934364319\n",
            "At 2000th batch    mean_loss=0.2487110197544098 , mean_accuracy=0.7937520742416382\n",
            "\n",
            "Time required for 5 th epoch : 95.5379478931427\n",
            "Validation loss :0.008321321569383144 Validation Accuracy: 0.7938432097434998 \n",
            "\n",
            "\n",
            "---Epoch 6  :----\n",
            "At 100th batch    mean_loss=0.24675598740577698 , mean_accuracy=0.7947343587875366\n",
            "At 200th batch    mean_loss=0.23291151225566864 , mean_accuracy=0.7956646680831909\n",
            "At 300th batch    mean_loss=0.2545590400695801 , mean_accuracy=0.7966085076332092\n",
            "At 400th batch    mean_loss=0.23020422458648682 , mean_accuracy=0.7975435256958008\n",
            "At 500th batch    mean_loss=0.22969159483909607 , mean_accuracy=0.79849773645401\n",
            "At 600th batch    mean_loss=0.2296496331691742 , mean_accuracy=0.799439013004303\n",
            "At 700th batch    mean_loss=0.20510224997997284 , mean_accuracy=0.8003970980644226\n",
            "At 800th batch    mean_loss=0.21253585815429688 , mean_accuracy=0.8013515472412109\n",
            "At 900th batch    mean_loss=0.19330966472625732 , mean_accuracy=0.8023176193237305\n",
            "At 1000th batch    mean_loss=0.18727412819862366 , mean_accuracy=0.8032795190811157\n",
            "At 1100th batch    mean_loss=0.18725895881652832 , mean_accuracy=0.8042591214179993\n",
            "At 1200th batch    mean_loss=0.18449905514717102 , mean_accuracy=0.8052283525466919\n",
            "At 1300th batch    mean_loss=0.18152494728565216 , mean_accuracy=0.806202232837677\n",
            "At 1400th batch    mean_loss=0.16514329612255096 , mean_accuracy=0.807172417640686\n",
            "At 1500th batch    mean_loss=0.161729097366333 , mean_accuracy=0.8081372976303101\n",
            "At 1600th batch    mean_loss=0.1618492305278778 , mean_accuracy=0.8091074228286743\n",
            "At 1700th batch    mean_loss=0.1584772914648056 , mean_accuracy=0.8100836873054504\n",
            "At 1800th batch    mean_loss=0.15184029936790466 , mean_accuracy=0.8110543489456177\n",
            "At 1900th batch    mean_loss=0.14342784881591797 , mean_accuracy=0.8120267987251282\n",
            "At 2000th batch    mean_loss=0.1413400024175644 , mean_accuracy=0.8129984736442566\n",
            "\n",
            "Time required for 6 th epoch : 95.8071506023407\n",
            "Validation loss :0.006851885933429003 Validation Accuracy: 0.8134138584136963 \n",
            "\n",
            "\n",
            "---Epoch 7  :----\n",
            "At 100th batch    mean_loss=0.1427432894706726 , mean_accuracy=0.8143197298049927\n",
            "At 200th batch    mean_loss=0.1351299285888672 , mean_accuracy=0.8152763247489929\n",
            "At 300th batch    mean_loss=0.1435188353061676 , mean_accuracy=0.8162261247634888\n",
            "At 400th batch    mean_loss=0.12439286708831787 , mean_accuracy=0.8171789050102234\n",
            "At 500th batch    mean_loss=0.11734972149133682 , mean_accuracy=0.818112850189209\n",
            "At 600th batch    mean_loss=0.13396012783050537 , mean_accuracy=0.8190288543701172\n",
            "At 700th batch    mean_loss=0.11148456484079361 , mean_accuracy=0.8199501037597656\n",
            "At 800th batch    mean_loss=0.12496379762887955 , mean_accuracy=0.8208563923835754\n",
            "At 900th batch    mean_loss=0.11137519031763077 , mean_accuracy=0.8217479586601257\n",
            "At 1000th batch    mean_loss=0.10904186964035034 , mean_accuracy=0.8226410746574402\n",
            "At 1100th batch    mean_loss=0.10948437452316284 , mean_accuracy=0.8235306739807129\n",
            "At 1200th batch    mean_loss=0.1147279292345047 , mean_accuracy=0.8244117498397827\n",
            "At 1300th batch    mean_loss=0.11023402214050293 , mean_accuracy=0.8252742886543274\n",
            "At 1400th batch    mean_loss=0.10554254055023193 , mean_accuracy=0.826135516166687\n",
            "At 1500th batch    mean_loss=0.09147597849369049 , mean_accuracy=0.8269988298416138\n",
            "At 1600th batch    mean_loss=0.09660850465297699 , mean_accuracy=0.8278568387031555\n",
            "At 1700th batch    mean_loss=0.09745677560567856 , mean_accuracy=0.8287081718444824\n",
            "At 1800th batch    mean_loss=0.09427469968795776 , mean_accuracy=0.8295577168464661\n",
            "At 1900th batch    mean_loss=0.0856313481926918 , mean_accuracy=0.8303945660591125\n",
            "At 2000th batch    mean_loss=0.09394562989473343 , mean_accuracy=0.8312265276908875\n",
            "\n",
            "Time required for 7 th epoch : 96.15110206604004\n",
            "Validation loss :0.005882082972675562 Validation Accuracy: 0.831658124923706 \n",
            "\n",
            "\n",
            "---Epoch 8  :----\n",
            "At 100th batch    mean_loss=0.09035392105579376 , mean_accuracy=0.8324364423751831\n",
            "At 200th batch    mean_loss=0.08698984980583191 , mean_accuracy=0.8332449793815613\n",
            "At 300th batch    mean_loss=0.0937613993883133 , mean_accuracy=0.8340455889701843\n",
            "At 400th batch    mean_loss=0.08929236233234406 , mean_accuracy=0.8348433375358582\n",
            "At 500th batch    mean_loss=0.08523745834827423 , mean_accuracy=0.8356243968009949\n",
            "At 600th batch    mean_loss=0.09535051137208939 , mean_accuracy=0.8363933563232422\n",
            "At 700th batch    mean_loss=0.07499564439058304 , mean_accuracy=0.8371682167053223\n",
            "At 800th batch    mean_loss=0.07968200743198395 , mean_accuracy=0.8379295468330383\n",
            "At 900th batch    mean_loss=0.0833408311009407 , mean_accuracy=0.838683009147644\n",
            "At 1000th batch    mean_loss=0.07487500458955765 , mean_accuracy=0.8394306898117065\n",
            "At 1100th batch    mean_loss=0.0759112760424614 , mean_accuracy=0.8401716351509094\n",
            "At 1200th batch    mean_loss=0.08051740378141403 , mean_accuracy=0.8409083485603333\n",
            "At 1300th batch    mean_loss=0.08171479403972626 , mean_accuracy=0.8416310548782349\n",
            "At 1400th batch    mean_loss=0.07965464890003204 , mean_accuracy=0.8423420190811157\n",
            "At 1500th batch    mean_loss=0.07143785059452057 , mean_accuracy=0.8430514335632324\n",
            "At 1600th batch    mean_loss=0.0736866146326065 , mean_accuracy=0.8437568545341492\n",
            "At 1700th batch    mean_loss=0.07877383381128311 , mean_accuracy=0.8444516062736511\n",
            "At 1800th batch    mean_loss=0.07402269542217255 , mean_accuracy=0.8451451063156128\n",
            "At 1900th batch    mean_loss=0.06553226709365845 , mean_accuracy=0.8458306193351746\n",
            "At 2000th batch    mean_loss=0.07114382088184357 , mean_accuracy=0.846515953540802\n",
            "\n",
            "Time required for 8 th epoch : 95.77231216430664\n",
            "Validation loss :0.005281811114400625 Validation Accuracy: 0.8468918204307556 \n",
            "\n",
            "\n",
            "---Epoch 9  :----\n",
            "At 100th batch    mean_loss=0.0710199624300003 , mean_accuracy=0.8475263714790344\n",
            "At 200th batch    mean_loss=0.06727132201194763 , mean_accuracy=0.8481917381286621\n",
            "At 300th batch    mean_loss=0.08080939948558807 , mean_accuracy=0.8488503694534302\n",
            "At 400th batch    mean_loss=0.07119117677211761 , mean_accuracy=0.8495014905929565\n",
            "At 500th batch    mean_loss=0.06488536298274994 , mean_accuracy=0.8501456379890442\n",
            "At 600th batch    mean_loss=0.08113308250904083 , mean_accuracy=0.8507781028747559\n",
            "At 700th batch    mean_loss=0.06207818537950516 , mean_accuracy=0.8514159917831421\n",
            "At 800th batch    mean_loss=0.07519073784351349 , mean_accuracy=0.8520461320877075\n",
            "At 900th batch    mean_loss=0.06687341630458832 , mean_accuracy=0.8526698350906372\n",
            "At 1000th batch    mean_loss=0.06205178052186966 , mean_accuracy=0.8532880544662476\n",
            "At 1100th batch    mean_loss=0.06391088664531708 , mean_accuracy=0.8539031744003296\n",
            "At 1200th batch    mean_loss=0.06756265461444855 , mean_accuracy=0.854514479637146\n",
            "At 1300th batch    mean_loss=0.06674305349588394 , mean_accuracy=0.8551190495491028\n",
            "At 1400th batch    mean_loss=0.06010527163743973 , mean_accuracy=0.8557214736938477\n",
            "At 1500th batch    mean_loss=0.060028642416000366 , mean_accuracy=0.8563095927238464\n",
            "At 1600th batch    mean_loss=0.059638362377882004 , mean_accuracy=0.8569031953811646\n",
            "At 1700th batch    mean_loss=0.06348935514688492 , mean_accuracy=0.8574885129928589\n",
            "At 1800th batch    mean_loss=0.07316933572292328 , mean_accuracy=0.8579902648925781\n",
            "At 1900th batch    mean_loss=0.058076709508895874 , mean_accuracy=0.8585638999938965\n",
            "At 2000th batch    mean_loss=0.058417677879333496 , mean_accuracy=0.8591403961181641\n",
            "\n",
            "Time required for 9 th epoch : 95.83565521240234\n",
            "Validation loss :0.004645532928407192 Validation Accuracy: 0.8595155477523804 \n",
            "\n",
            "\n",
            "---Epoch 10  :----\n",
            "At 100th batch    mean_loss=0.056894607841968536 , mean_accuracy=0.8600567579269409\n",
            "At 200th batch    mean_loss=0.05611782521009445 , mean_accuracy=0.8606199026107788\n",
            "At 300th batch    mean_loss=0.06900381296873093 , mean_accuracy=0.8611786961555481\n",
            "At 400th batch    mean_loss=0.05860035866498947 , mean_accuracy=0.8617309331893921\n",
            "At 500th batch    mean_loss=0.05772056058049202 , mean_accuracy=0.8622770309448242\n",
            "At 600th batch    mean_loss=0.0692819207906723 , mean_accuracy=0.8628131747245789\n",
            "At 700th batch    mean_loss=0.054067712277173996 , mean_accuracy=0.8633513450622559\n",
            "At 800th batch    mean_loss=0.057674191892147064 , mean_accuracy=0.8638849258422852\n",
            "At 900th batch    mean_loss=0.06051148101687431 , mean_accuracy=0.8644139170646667\n",
            "At 1000th batch    mean_loss=0.05250449851155281 , mean_accuracy=0.864936351776123\n",
            "At 1100th batch    mean_loss=0.05123937129974365 , mean_accuracy=0.8654549717903137\n",
            "At 1200th batch    mean_loss=0.057849984616041183 , mean_accuracy=0.8659694790840149\n",
            "At 1300th batch    mean_loss=0.05796077847480774 , mean_accuracy=0.8664795160293579\n",
            "At 1400th batch    mean_loss=0.05276120454072952 , mean_accuracy=0.8669837117195129\n",
            "At 1500th batch    mean_loss=0.0589425191283226 , mean_accuracy=0.8674800992012024\n",
            "At 1600th batch    mean_loss=0.05254298448562622 , mean_accuracy=0.8679771423339844\n",
            "At 1700th batch    mean_loss=0.05786920711398125 , mean_accuracy=0.8684674501419067\n",
            "At 1800th batch    mean_loss=0.050357937812805176 , mean_accuracy=0.8689579963684082\n",
            "At 1900th batch    mean_loss=0.053843624889850616 , mean_accuracy=0.8694416284561157\n",
            "At 2000th batch    mean_loss=0.05592479556798935 , mean_accuracy=0.8699247241020203\n",
            "\n",
            "Time required for 10 th epoch : 96.78116130828857\n",
            "Validation loss :0.004483766388148069 Validation Accuracy: 0.8702208399772644 \n",
            "\n",
            "\n",
            "---Epoch 11  :----\n",
            "At 100th batch    mean_loss=0.0505218580365181 , mean_accuracy=0.8706744313240051\n",
            "At 200th batch    mean_loss=0.05003040283918381 , mean_accuracy=0.8711479306221008\n",
            "At 300th batch    mean_loss=0.06450740993022919 , mean_accuracy=0.8716171383857727\n",
            "At 400th batch    mean_loss=0.05480257421731949 , mean_accuracy=0.8720836639404297\n",
            "At 500th batch    mean_loss=0.05225718766450882 , mean_accuracy=0.8725439310073853\n",
            "At 600th batch    mean_loss=0.06331438571214676 , mean_accuracy=0.8729960322380066\n",
            "At 700th batch    mean_loss=0.049482110887765884 , mean_accuracy=0.8734481930732727\n",
            "At 800th batch    mean_loss=0.05365768074989319 , mean_accuracy=0.8738978505134583\n",
            "At 900th batch    mean_loss=0.36782974004745483 , mean_accuracy=0.8743389844894409\n",
            "At 1000th batch    mean_loss=0.05408342927694321 , mean_accuracy=0.8746564984321594\n",
            "At 1100th batch    mean_loss=0.04947127774357796 , mean_accuracy=0.875093936920166\n",
            "At 1200th batch    mean_loss=0.05346464365720749 , mean_accuracy=0.8755325078964233\n",
            "At 1300th batch    mean_loss=0.04892103374004364 , mean_accuracy=0.8759668469429016\n",
            "At 1400th batch    mean_loss=0.046980760991573334 , mean_accuracy=0.8763977289199829\n",
            "At 1500th batch    mean_loss=0.046747516840696335 , mean_accuracy=0.8768264651298523\n",
            "At 1600th batch    mean_loss=0.04641047865152359 , mean_accuracy=0.877253532409668\n",
            "At 1700th batch    mean_loss=0.05124685913324356 , mean_accuracy=0.87767493724823\n",
            "At 1800th batch    mean_loss=0.045126620680093765 , mean_accuracy=0.8780954480171204\n",
            "At 1900th batch    mean_loss=0.04383949562907219 , mean_accuracy=0.8785114288330078\n",
            "At 2000th batch    mean_loss=0.047122396528720856 , mean_accuracy=0.8789262771606445\n",
            "\n",
            "Time required for 11 th epoch : 96.3066873550415\n",
            "Validation loss :0.00434877397492528 Validation Accuracy: 0.8791661858558655 \n",
            "\n",
            "\n",
            "---Epoch 12  :----\n",
            "At 100th batch    mean_loss=0.04607916250824928 , mean_accuracy=0.8795545697212219\n",
            "At 200th batch    mean_loss=0.0455855093896389 , mean_accuracy=0.879960834980011\n",
            "At 300th batch    mean_loss=0.05764595791697502 , mean_accuracy=0.8803631663322449\n",
            "At 400th batch    mean_loss=0.04923436790704727 , mean_accuracy=0.8807629346847534\n",
            "At 500th batch    mean_loss=0.047007206827402115 , mean_accuracy=0.8811588287353516\n",
            "At 600th batch    mean_loss=0.05722871422767639 , mean_accuracy=0.8815473318099976\n",
            "At 700th batch    mean_loss=0.046413857489824295 , mean_accuracy=0.8819364309310913\n",
            "At 800th batch    mean_loss=0.0493638776242733 , mean_accuracy=0.8823220133781433\n",
            "At 900th batch    mean_loss=0.04708481207489967 , mean_accuracy=0.8827061653137207\n",
            "At 1000th batch    mean_loss=0.0442514531314373 , mean_accuracy=0.8830872774124146\n",
            "At 1100th batch    mean_loss=0.04721778631210327 , mean_accuracy=0.8834622502326965\n",
            "At 1200th batch    mean_loss=0.05160684138536453 , mean_accuracy=0.883834958076477\n",
            "At 1300th batch    mean_loss=0.04706292599439621 , mean_accuracy=0.8842061758041382\n",
            "At 1400th batch    mean_loss=0.04862300679087639 , mean_accuracy=0.8845729827880859\n",
            "At 1500th batch    mean_loss=0.04529687762260437 , mean_accuracy=0.8849395513534546\n",
            "At 1600th batch    mean_loss=0.045488547533750534 , mean_accuracy=0.8853051066398621\n",
            "At 1700th batch    mean_loss=0.04972085729241371 , mean_accuracy=0.8856661915779114\n",
            "At 1800th batch    mean_loss=0.0434948094189167 , mean_accuracy=0.8860241770744324\n",
            "At 1900th batch    mean_loss=0.044859208166599274 , mean_accuracy=0.8863809704780579\n",
            "At 2000th batch    mean_loss=0.04439472407102585 , mean_accuracy=0.886737048625946\n",
            "\n",
            "Time required for 12 th epoch : 96.0106737613678\n",
            "Validation loss :0.004507753998041153 Validation Accuracy: 0.8868937492370605 \n",
            "\n",
            "\n",
            "---Epoch 13  :----\n",
            "At 100th batch    mean_loss=0.0438033789396286 , mean_accuracy=0.887228786945343\n",
            "At 200th batch    mean_loss=0.04303356632590294 , mean_accuracy=0.8875789642333984\n",
            "At 300th batch    mean_loss=0.05579230934381485 , mean_accuracy=0.8879254460334778\n",
            "At 400th batch    mean_loss=0.049205828458070755 , mean_accuracy=0.8882627487182617\n",
            "At 500th batch    mean_loss=0.04277610033750534 , mean_accuracy=0.8886045217514038\n",
            "At 600th batch    mean_loss=0.05383904650807381 , mean_accuracy=0.8889402151107788\n",
            "At 700th batch    mean_loss=0.04456373676657677 , mean_accuracy=0.889277458190918\n",
            "At 800th batch    mean_loss=0.05083732679486275 , mean_accuracy=0.8896061182022095\n",
            "At 900th batch    mean_loss=0.04207976534962654 , mean_accuracy=0.8899393081665039\n",
            "At 1000th batch    mean_loss=0.04100459814071655 , mean_accuracy=0.8902714848518372\n",
            "At 1100th batch    mean_loss=0.041208431124687195 , mean_accuracy=0.8905998468399048\n",
            "At 1200th batch    mean_loss=0.04735087603330612 , mean_accuracy=0.8909246325492859\n",
            "At 1300th batch    mean_loss=0.044422779232263565 , mean_accuracy=0.8912480473518372\n",
            "At 1400th batch    mean_loss=0.04438602179288864 , mean_accuracy=0.8915321230888367\n",
            "At 1500th batch    mean_loss=0.041898973286151886 , mean_accuracy=0.8918507695198059\n",
            "At 1600th batch    mean_loss=0.04049227386713028 , mean_accuracy=0.8921705484390259\n",
            "At 1700th batch    mean_loss=0.0445537343621254 , mean_accuracy=0.892487645149231\n",
            "At 1800th batch    mean_loss=0.04003506898880005 , mean_accuracy=0.8928017616271973\n",
            "At 1900th batch    mean_loss=0.03789491578936577 , mean_accuracy=0.8931146860122681\n",
            "At 2000th batch    mean_loss=0.03833750635385513 , mean_accuracy=0.8934259414672852\n",
            "\n",
            "Time required for 13 th epoch : 90.28107047080994\n",
            "Validation loss :0.004380778409540653 Validation Accuracy: 0.8935580253601074 \n",
            "\n",
            "\n",
            "---Epoch 14  :----\n",
            "At 100th batch    mean_loss=0.04066114500164986 , mean_accuracy=0.8938522338867188\n",
            "At 200th batch    mean_loss=0.04061443358659744 , mean_accuracy=0.8941589593887329\n",
            "At 300th batch    mean_loss=0.05178726837038994 , mean_accuracy=0.8944622874259949\n",
            "At 400th batch    mean_loss=0.044133104383945465 , mean_accuracy=0.8947649002075195\n",
            "At 500th batch    mean_loss=0.041613854467868805 , mean_accuracy=0.8950651288032532\n",
            "At 600th batch    mean_loss=0.053545109927654266 , mean_accuracy=0.8953585624694824\n",
            "At 700th batch    mean_loss=0.042230140417814255 , mean_accuracy=0.8956521153450012\n",
            "At 800th batch    mean_loss=0.04293940216302872 , mean_accuracy=0.8959445953369141\n",
            "At 900th batch    mean_loss=0.04113585501909256 , mean_accuracy=0.8962299227714539\n",
            "At 1000th batch    mean_loss=0.04055920988321304 , mean_accuracy=0.8965207934379578\n",
            "At 1100th batch    mean_loss=0.03878067061305046 , mean_accuracy=0.8968088030815125\n",
            "At 1200th batch    mean_loss=0.04307868331670761 , mean_accuracy=0.8970952033996582\n",
            "At 1300th batch    mean_loss=0.04091952368617058 , mean_accuracy=0.8973787426948547\n",
            "At 1400th batch    mean_loss=0.037645064294338226 , mean_accuracy=0.8976607322692871\n",
            "At 1500th batch    mean_loss=0.04061511904001236 , mean_accuracy=0.8979430198669434\n",
            "At 1600th batch    mean_loss=0.038158055394887924 , mean_accuracy=0.8982251286506653\n",
            "At 1700th batch    mean_loss=0.042994361370801926 , mean_accuracy=0.8985029458999634\n",
            "At 1800th batch    mean_loss=0.04196108132600784 , mean_accuracy=0.898776650428772\n",
            "At 1900th batch    mean_loss=0.03913108631968498 , mean_accuracy=0.8990478515625\n",
            "At 2000th batch    mean_loss=0.0375652089715004 , mean_accuracy=0.8993207812309265\n",
            "\n",
            "Time required for 14 th epoch : 96.75483679771423\n",
            "Validation loss :0.004477357026189566 Validation Accuracy: 0.8994060158729553 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "wgS3BXMBFyVi",
        "outputId": "3adb72eb-83f3-49cd-eb81-cf99322ebbcf"
      },
      "source": [
        "# checking the attention matrix for sample input\r\n",
        "\r\n",
        "inp=train_input[5].reshape((1,15))\r\n",
        "out=train_output[5]\r\n",
        "\r\n",
        "p=Prediction(1)\r\n",
        "\r\n",
        "ypred,weight=p(inp)\r\n",
        "weight=tf.reshape(weight,shape=(15,21))\r\n",
        "fig = plt.figure(figsize=(10,10))\r\n",
        "ax = fig.add_subplot(1, 1, 1)\r\n",
        "ax.matshow(weight, cmap='viridis')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjv1ClMqMR3Z"
      },
      "source": [
        "# The above plot shows that attention model has learnt the weights (word alignment) correctly."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD6VkL7yLtJr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}